{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Web Scraping, Part Two (Python)\n",
    "- UMN LATIS & Libraries workshop, Nov 15, 2019\n",
    "- Cody Hennesy (chennesy@umn.edu) and Michael Beckstrand (mjbeckst@umn.edu)\n",
    "\n",
    "In this part of the workshop, we'll explore reproducible web scraping methods using Python. \n",
    "\n",
    "Specifically, in this part of the workshop we will:\n",
    "* Use Python 3 in a JupyterLab computing environment\n",
    "* Use the Requests and BeautifulSoup Python libraries to access HTML data from the web\n",
    "* Create variables, lists and loops to work with web data in Python\n",
    "* Store and view HTML data in Pandas dataframe format\n",
    "\n",
    "Credits: Content for this workshop was adapted from [Rochelle Terman's Web Scraping workshop](https://github.com/rochelleterman/scrape-interwebz) and from [Software Carpentry Python lessons](http://swcarpentry.github.io/python-novice-inflammation/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Python? \n",
    "- Reproducibility\n",
    "- Repeatable\n",
    "- Extensible\n",
    "- Great for data access and data cleaning\n",
    "\n",
    "### What's Jupyter?\n",
    "- Web-based, easy to share\n",
    "- Easy to read, easy to run\n",
    "- Run code piece by piece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python variables\n",
    "- You can use Python as a calculator. \n",
    "- To \"run\" a Jupyter cell hold down shift and select Return/Enter, or choose the \"play icon\" (right-facing triangle) from the Jupyter menu above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_kg = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weight_kg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, variable names:\n",
    "\n",
    "* can include letters, digits, and underscores\n",
    "* cannot start with a digit\n",
    "* are case sensitive.\n",
    "\n",
    "You can do calculations and save text strings in variables too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website = \"All the words on a website\"\n",
    "print(website)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "Importing a library is like getting a piece of lab equipment out of a storage locker and setting it up on the bench. Libraries provide additional functionality to the basic Python package, much like a new piece of equipment adds functionality to a lab space. Just like in the lab, importing too many libraries can sometimes complicate and slow down your programs - so we only import what we need for each program.\n",
    "\n",
    "Our primary tools will be the [Requests library](http://docs.python-requests.org/en/latest/user/quickstart/)\n",
    "and [Beautiful Soup](http://www.crummy.com/software/BeautifulSoup/bs4/doc/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library functions\n",
    "The expression ```requests.get(...)``` is a function call that asks Python to run the function ```get``` which belongs to the ```requests``` library. \n",
    "\n",
    "This dotted notation is used everywhere in Python: the thing that appears before the dot contains the thing that appears after.\n",
    "\n",
    "As an example, we could use the dot notation to write the relationship between Minneapolis and Minnesota as ```Minnesota.Minneapolis```, just as *get* is a function that belongs to the *requests* library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get('http://www.startribune.com/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What did we do above?\n",
    "1. Created a Python HTTP request object for a GET\n",
    "2. Send the HTTP request to webserver at http://www.startribune.com/\n",
    "3. Received the response ```[200]``` from http://www.startribune.com/ - [what's that mean?](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Jupyter notebooks using Python you can explore functions of a library using the *tab* key.\n",
    "\n",
    "And to understand each function you can get information by putting a question mark after it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can store the data that is returned from the GET request in a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_trib = requests.get('http://www.startribune.com/')\n",
    "print(star_trib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ethical scraping\n",
    "One way to make sure you're engaging in transparent and ethical scraping practices is to send the website information about *yourself* along with your request. \n",
    "\n",
    "```requests.get``` includes a ```headers=``` parameter that you can use to send in your name and information about the software we're using to collect data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user-agent': 'python-requests/2.22.0; chennesy@umn.edu; Cody Hennesy'}\n",
    "star_trib = requests.get('http://www.startribune.com/', headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can explore the attributes of the data object stored in ```star_trib``` using the same dot notation. \n",
    "\n",
    "Use tab to explore the options, and the question mark to read more about the attribute.\n",
    "\n",
    "```star_trib.text```, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = star_trib.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move the .text content that was returned from the Request into a BeautifulSoup object so we can start to explore the HTML tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the response into an HTML tree by calling BeautifulSoup\n",
    "soup = BeautifulSoup(src, 'lxml')\n",
    "\n",
    "# look at what it looks like now, using the soup.prettify tool\n",
    "# [:1000] will give us the first 1000 characters in the soup object so it doesn't fill up the whole screen\n",
    "print(soup.prettify()[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Elements\n",
    "\n",
    "BeautifulSoup has a number of functions to find things on a page. Like other webscraping tools, Beautiful Soup lets you find elements by their:\n",
    "\n",
    "1. HTML tags\n",
    "2. HTML Attributes\n",
    "3. CSS Selectors\n",
    "\n",
    "#### HTML tags\n",
    "Let's search first for **HTML tags**. \n",
    "\n",
    "The function `find_all` searches the `soup` tree to find all the elements with an a particular HTML tag, and returns all of those elements.\n",
    "\n",
    "What does the example below do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all elements in a certain tag\n",
    "#soup.find_all(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soup.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soup.find_all('h3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because `find_all()` is the most popular method in the Beautiful Soup search API, you can use a shortcut for it. If you treat the BeautifulSoup object as though it were a function, then itâ€™s the same as calling `find_all()` on that object. \n",
    "\n",
    "These two lines of code are equivalent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup.find_all(\"a\")\n",
    "# soup(\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HTML Attributes \n",
    "If you search for everything with the `a` tag, you're likely to get a lot of stuff, much of which you don't want. What if we wanted to search for HTML tags ONLY with certain attributes, like particular CSS classes? \n",
    "\n",
    "We can do this by adding an additional argument to the `find_all` like this: ```soup(\"a\", class_=\"class_name\")```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1: Find the Most Read and Emailed articles on the Star Tribune homepage\n",
    "Use Chrome's *Inspect* feature, to find the class name for the Most Read and Most Emailed articles lists. \n",
    "1. Create a variable called most_read, and use ```soup()``` to find all of the links with the appropriate class\n",
    "2. Print out the matches below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below, we are finding all the `a` tags, and then filtering those with `class_=\"feed-list-link\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only the 'a' tags in 'sidemenu' class\n",
    "most_read = soup(\"a\", class_=\"feed-list-link\")\n",
    "most_read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSS Selectors\n",
    "It can be more efficient to search and find things on a website by **CSS selector.** For this we have to use a different method, `select()`. Just pass a string into the `.select()` to get all elements with that string as a valid CSS selector.\n",
    "\n",
    "In the example above, we can use \"a.feed-list-link\" as a CSS selector, which returns all `a` tags with class `feed-list-link`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get elements with \"a.sidemenu\" CSS Selector.\n",
    "most_read_select = soup.select(\"a.feed-list-link\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#most_read_select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Lists\n",
    "\n",
    "The most popular kind of data collection in Python is the list, which takes the place of arrays in programming languages like C and Fortran.\n",
    "Lists have two primary important characteristics:\n",
    "1. They are mutable, i.e., they can be changed after they are created.\n",
    "2. They are heterogeneous, i.e., they can store values of many different types.\n",
    "\n",
    "To create a new list, you can just put some values in square brackets with commas in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = ['red', 'orange', 'yellow']\n",
    "my_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fetch the element at a specific location, put the *index* of that location in square brackets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to all of the a links with the class selector ```feed-list-link```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a Python list\n",
    "most_read = soup.select(\"a.feed-list-link\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how many items are in your list using the ```len()``` Python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(most_read)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can look at the first element in the list using the syntax variable[0]. \n",
    "\n",
    "Note: [0] refers to the first element in a list in Python, and [1] refers to the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_read[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a built in Python function called type() to explore the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the first element in the list to its own variable to make it easier to explore\n",
    "first_link = most_read[0]\n",
    "\n",
    "# check out its class\n",
    "type(first_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a tag! If we look up Tag in the BeautifulSoup documentation, we know that we can use `.text` to look at the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_link.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the href attribute to check out the URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_link['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loops\n",
    "\n",
    "If we want to explore all of the most popular articles, we can loop through each link and only grab the information that we care about. \n",
    "\n",
    "#### Note the syntax: \n",
    "\n",
    "```\n",
    "for x in y:\n",
    "    do_something # the code in the loop needs to be indented\n",
    "    do_another_thing\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 'a' tags, we also know there's an 'href' attribute that tells us where the link URL goes. \n",
    "To look at all of the attributes, we can call .attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in most_read:\n",
    "    print(link.text, link['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's clean that up a bit:\n",
    "for link in most_read:\n",
    "    print(link.text.strip(), '\\n', link['href'], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2: Find all of the a tags\n",
    "Remember when we used soup.findall to collect all of the links on the Star Tribune homepage? \n",
    "Create a variable to collect all of the links on the homepage, and then loop over the list to print the text of each link and the URL?\n",
    "\n",
    "Hint: the results might look pretty messy! You can use ```.strip()``` to remove whitespace from strings and make the output easier on the eyes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = soup.find_all(\"a\")\n",
    "for link in all_links:\n",
    "    print(link.text.strip(), link['href'].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at a specific article page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(\"http://www.startribune.com/discover-lost-shipwrecks-in-lake-superior/514224542/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = page.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_soup = BeautifulSoup(src, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the HTML in Chrome is a great way to find the right selectors or attributes to scrape, but you can also take sneak peaks at common tags using ```.find_all()``` to help pinpoint specific elements. For example: ```.find_all('h1')``` or ```.find_all('p')```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#page_soup.find_all('p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the p class ```Text_Body_mag``` would snag the full-text of the article for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = page_soup.select(\"p.Text_Body_mag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in article_text:\n",
    "    print(article.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 3: Scrape the headline, byline, and date\n",
    "Explore the HTML for a Star Tribune article to see if you can scrape the headline, byline (author), and the date the article was posted from the page.\n",
    "\n",
    "- Hint: there are several different routes to get to each element. \n",
    "- Hint two: The date and bylines can be a little confusing when you use ```.select()``` because they'll return a Python list, even if there's only one item on the list. To show the ```.text``` attribute from a list, you can point to the first item on the list using ```[0]```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline = page_soup.h1\n",
    "headline.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "byline = page_soup.select('div.article-byline')\n",
    "byline[0].a.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = page_soup.select('div.article-dateline')\n",
    "date[0].text.strip()[:-9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
