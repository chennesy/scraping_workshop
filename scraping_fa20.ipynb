{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Web Scraping, Part Two (Python)\n",
    "- UMN LATIS & Libraries workshop, Nov 6, 2020\n",
    "- Cody Hennesy (chennesy@umn.edu) and Michael Beckstrand (mjbeckst@umn.edu)\n",
    "\n",
    "In this part of the workshop, we'll explore reproducible web scraping methods using Python. \n",
    "\n",
    "Specifically, in this part of the workshop we will:\n",
    "* Use Python 3 in a Jupyter computing environment\n",
    "* Use the Requests and BeautifulSoup Python libraries to access HTML data from the web\n",
    "* Create variables, lists and loops to work with web data in Python\n",
    "\n",
    "Credits: Content for this workshop was adapted from [Rochelle Terman's Web Scraping workshop](https://github.com/rochelleterman/scrape-interwebz) and from [Software Carpentry Python lessons](http://swcarpentry.github.io/python-novice-inflammation/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Python? \n",
    "- Reproducibility\n",
    "- Repeatable\n",
    "- Extensible\n",
    "- Great for data access and data cleaning\n",
    "\n",
    "### What's Jupyter?\n",
    "- Web-based, easy to share\n",
    "- Easy to read, easy to run\n",
    "- Run code piece by piece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python variables\n",
    "- You can use Python as a calculator. \n",
    "- To \"run\" a Jupyter cell hold down shift and select Return/Enter, or choose the \"play icon\" (right-facing triangle) from the Jupyter menu above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_kg = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weight_kg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, variable names:\n",
    "\n",
    "* can include letters, digits, and underscores\n",
    "* cannot start with a digit\n",
    "* are case sensitive.\n",
    "\n",
    "You can do calculations and save text strings in variables too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website = \"All the words on a website\"\n",
    "print(website)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "Importing a library is like getting a piece of lab equipment out of a storage locker and setting it up on the bench. Libraries provide additional functionality to the basic Python package, much like a new piece of equipment adds functionality to a lab space. Just like in the lab, importing too many libraries can sometimes complicate and slow down your programs - so we only import what we need for each program.\n",
    "\n",
    "Our primary tools will be the [Requests library](http://docs.python-requests.org/en/latest/user/quickstart/)\n",
    "and [Beautiful Soup](http://www.crummy.com/software/BeautifulSoup/bs4/doc/). \n",
    "\n",
    "Note: Another popular tool for web scraping with Python is [Scrapy](https://scrapy.org/). The general consensus is that it's a faster tool, and it does *more* than Beautiful Soup, but it might be more complex to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library functions\n",
    "The expression ```requests.get(...)``` is a function call that asks Python to run the function ```get``` which belongs to the ```requests``` library. \n",
    "\n",
    "This dotted notation is used everywhere in Python: the thing that appears before the dot contains the thing that appears after.\n",
    "\n",
    "As an example, we could use the dot notation to write the relationship between Minneapolis and Minnesota as ```Minnesota.Minneapolis```, just as *get* is a function that belongs to the *requests* library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get('http://www.startribune.com/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What did we do above?\n",
    "1. Created a Python HTTP request object for a GET\n",
    "2. Send the HTTP request to webserver at http://www.startribune.com/\n",
    "3. Received the response ```[200]``` from http://www.startribune.com/ - [what's that mean?](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Jupyter notebooks using Python you can explore functions of a library using the *tab* key.\n",
    "\n",
    "And to understand each function you can get information by putting a question mark after it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can store the data that is returned from the GET request in a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_trib = requests.get('http://www.startribune.com/')\n",
    "#print(star_trib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ethical scraping\n",
    "One way to make sure you're engaging in transparent and ethical scraping practices is to send the website information about *yourself* along with your request. \n",
    "\n",
    "```requests.get``` includes a ```headers=``` parameter that you can use to send in your name and information about the software we're using to collect data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user-agent': 'python-requests/2.22.0; chennesy@umn.edu; Cody Hennesy'}\n",
    "star_trib = requests.get('http://www.startribune.com/', headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can explore the attributes of the data object stored in ```star_trib``` using the same dot notation. \n",
    "\n",
    "Use tab to explore the options, and the question mark to read more about the attribute.\n",
    "\n",
    "```star_trib.text```, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = star_trib.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move the .text content that was returned from the Request into a BeautifulSoup object so we can start to explore the HTML tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the response into an HTML tree by calling BeautifulSoup\n",
    "soup = BeautifulSoup(src, 'lxml')\n",
    "\n",
    "# look at what it looks like now, using the soup.prettify tool\n",
    "# [:1000] will give us the first 1000 characters in the soup object so it doesn't fill up the whole screen\n",
    "print(soup.prettify()[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Elements\n",
    "\n",
    "BeautifulSoup has a number of functions to find things on a page. Like other webscraping tools, Beautiful Soup lets you find elements by their:\n",
    "\n",
    "1. HTML tags\n",
    "2. HTML Attributes\n",
    "3. CSS Selectors\n",
    "\n",
    "#### HTML tags\n",
    "Let's search first for **HTML tags**. \n",
    "\n",
    "The function `find_all` searches the `soup` tree to find all the elements with an a particular HTML tag, and returns all of those elements.\n",
    "\n",
    "What does the example below do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all elements in a certain tag\n",
    "soup.find_all(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('h3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because `find_all()` is the most popular method in the Beautiful Soup search API, you can use a shortcut for it. If you treat the BeautifulSoup object as though it were a function, then itâ€™s the same as calling `find_all()` on that object. \n",
    "\n",
    "These two lines of code are equivalent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup.find_all(\"a\")\n",
    "soup(\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HTML Attributes \n",
    "If you search for everything with the `a` tag, you're likely to get a lot of stuff, much of which you don't want. What if we wanted to search for HTML tags ONLY with certain attributes, like particular CSS classes? \n",
    "\n",
    "We can do this by adding an additional argument to the `find_all` like this: ```soup(\"a\", class_=\"class_name\")```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge (1): Finding the Most Read and Emailed articles on the Star Tribune homepage\n",
    "We can use Chrome's *Inspect* feature, to find the class name for the Most Read and Most Emailed articles lists (feed-list-link). \n",
    "1. Let's create a variable called most_read, and use ```soup()``` to find all of the links with the appropriate class\n",
    "2. Then we'll print out the matches below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below, we are finding all the `a` tags, and then filtering those with `class_=\"feed-list-link\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only the 'a' tags in 'sidemenu' class\n",
    "most_read = soup(\"a\", class_=\"feed-list-link\")\n",
    "print(most_read)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSS Selectors\n",
    "It can be more efficient to search and find things on a website by **CSS selector.** For this we have to use a different method, `select()`. Just pass a string into the `.select()` to get all elements with that string as a valid CSS selector.\n",
    "\n",
    "In the example above, we can use \"a.feed-list-link\" as a CSS selector, which returns all `a` tags with class `feed-list-link`.\n",
    "\n",
    "##### How to find selectors?\n",
    "A number of browser extensions and other tools exist to help you find HTML and CSS. \n",
    "- [Selector Gadget](https://selectorgadget.com/)\n",
    "- [CSS Selector Helper](https://chrome.google.com/webstore/detail/css-selector-helper-for-c/gddgceinofapfodcekopkjjelkbjodin?hl=en)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get elements with \"a.sidemenu\" CSS Selector.\n",
    "most_read_select = soup.select(\"a.feed-list-link\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_read_select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Lists\n",
    "\n",
    "The most popular kind of data collection in Python is the list, which takes the place of arrays in programming languages like C and Fortran.\n",
    "Lists have two primary important characteristics:\n",
    "1. They are mutable, i.e., they can be changed after they are created.\n",
    "2. They are heterogeneous, i.e., they can store values of many different types.\n",
    "\n",
    "To create a new list, you can just put some values in square brackets with commas in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = ['red', 'orange', 'yellow']\n",
    "my_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fetch the element at a specific location, put the *index* of that location in square brackets. But keep in mind that Python lists start the index from 0. So the list above has three index values: ```my_list[0] my_list[1] my_list[2]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list[1]\n",
    "my_list[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to all of the a links with the class selector ```feed-list-link```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a Python list\n",
    "most_read = soup.select(\"a.feed-list-link\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how many items are in your list using the ```len()``` Python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(most_read)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can look at the first element in the list using the syntax variable[0]. \n",
    "\n",
    "Note: [0] refers to the first element in a list in Python, and [1] refers to the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_read[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a built in Python function called type() to explore the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the first element in the list to its own variable to make it easier to explore\n",
    "first_link = most_read[0]\n",
    "\n",
    "# check out its class\n",
    "type(first_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a tag! If we look up Tag in the BeautifulSoup documentation, we know that we can use `.text` to look at the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_link.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the href attribute to check out the URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_link['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loops\n",
    "\n",
    "If we want to explore all of the most popular articles, we can loop through each link and only grab the information that we care about. \n",
    "\n",
    "#### Note the syntax: \n",
    "\n",
    "```\n",
    "for x in y:\n",
    "    do_something # the code in the loop needs to be indented\n",
    "    do_another_thing\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 'a' tags, we also know there's an 'href' attribute that tells us where the link URL goes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll clean up the print output in Python by using a built-in .strip() function that removes extra white space from strings, and by adding some line breaks between elements using the ```'\\n'``` escape character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in most_read:\n",
    "    print(link.text.strip(), '\\n', link['href'], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While printing content can provide a useful output as we code, it's usually much more useful to store the data so that we can operate on it later on (save it, clean it, etc.). In this case let's create an empty Python list and then extract the URLs from our most_read list to save there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can create an empty list using open and closed square brackets without content\n",
    "most_read_urls = []\n",
    "\n",
    "for link in most_read:\n",
    "    # we can append new items to a list \"in place\" (without using an equals sign) using the append() function\n",
    "    most_read_urls.append(link['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(most_read_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list of URLs that we can later use as part of a modular approach to scraping. If we figure out how to scrape the content from an article page, we can re-use that code and say \"scrape the article content for every page in this list of URLs.\" To do that, let's figure out how to scrape content from an article page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape an article page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(\"https://www.startribune.com/former-vikings-great-matt-blair-dies-at-age-70-likely-linked-to-cte/572832852/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = page.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_soup = BeautifulSoup(src, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the HTML in Chrome is a great way to find the right selectors or attributes to scrape, but you can also take sneak peaks at common tags using ```.find_all()``` to help pinpoint specific elements. For example: ```.find_all('h1')``` or ```.find_all('p')```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_soup.find_all('p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the p class ```Text_Body``` would snag the full-text of the article for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = page_soup.select(\"p.Text_Body\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in article_text:\n",
    "    print(article.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments on Star Tribune - How to get them?\n",
    "\n",
    "Some elements of websites are \"hidden\" from web scrapers like BeautifulSoup because they appear as part of an iFrame, or because they require other code such as Javascript to load on the page. \n",
    "\n",
    "If we look at the link to \"Show Comments\" on the Star Tribune article, for example, there's not a URL, but a call to a Javascript tool called *js-comments* that is visible in the a class selector:\n",
    "\n",
    "```<a href=\"#\" class=\"js-comments-show comments-count-link talk-enabled\"><div class=\"comments-count\">18</div><span class=\"comments-show js-comments-show-txt\">Show Comments</span><img class=\"comment-count-image-tracking\" alt=\"\" src=\"http://apps.startribune.com/circulars/images/blank.gif\" style=\"display: none;\"></a>```\n",
    "\n",
    "To capture this \"hidden\" data, some researchers use browser emulators such as:\n",
    "\n",
    "- [Selenium](https://pypi.org/project/selenium/). This also requires other tools such as [ChromeDriver](https://sites.google.com/a/chromium.org/chromedriver/downloads), [RSelenium](https://cran.r-project.org/web/packages/RSelenium/index.html), and/or [Selenium with Python](https://selenium-python.readthedocs.io/).\n",
    "- [Puppeteer](https://pptr.dev/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge (3): Scrape the headline, byline, and date\n",
    "Let's explore the HTML for a Star Tribune article to see if we can scrape the headline, byline (author), and the date the article was posted from the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline = page_soup.h1\n",
    "headline.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "byline = page_soup.select('div.article-byline')\n",
    "byline[0].a.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = page_soup.select('div.article-dateline')\n",
    "date[0].text.strip()[:-9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "Now let's define a function ```scrape_articles``` that cycles through our ```most_read_urls``` list, and grabs all of the data we care about from each page.\n",
    "\n",
    "The function definition opens with the keyword ```def``` followed by the name of the function (format_articles) and a parenthesized list of parameter names (unformatted_docs). The body of the function â€” the statements that are executed when it runs â€” is indented below the definition line. The body concludes with a return keyword followed by the value we want to take from the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_things(x, y):\n",
    "    z = x + y\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the function, passing values to the x and y parameters, and save the return value to a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = add_things(10,25)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together: Scraping list function\n",
    "So let's use a function to scrape a few key elements from article pages. The input that the function will take is a list of URLs. Then for each url, it will scrape the headline, byline, and article text. Since we'll be hitting the Star Tribune server pretty rapidly, let's build in a timer using the time library to pause between each page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new function that accepts one parameter, a list.\n",
    "def scrape_strib(url_list):\n",
    "    #empty list to hold each page\n",
    "    results_list = []\n",
    "    \n",
    "    for url in url_list:\n",
    "        print('Scraping:', url)\n",
    "        time.sleep(5) # wait for 5 seconds\n",
    "        \n",
    "        # request the page content and convert it to a soup object\n",
    "        page = requests.get(url)\n",
    "        src = page.text\n",
    "        page_soup = BeautifulSoup(src, 'lxml')\n",
    "        \n",
    "        #find the text on the page\n",
    "        article_text = page_soup.select(\"p\") # because different kinds of articles use different p classes, we'll make this as generic as possible\n",
    "\n",
    "        # save all of the p_tags to a long string\n",
    "        article_string = ''.join(article.text.strip() for article in article_text)\n",
    "        \n",
    "        # save the headline (there should only be one!)\n",
    "        headline = page_soup.h1\n",
    "        \n",
    "        # we haven't talked about if/else statements, but before assigning a value we want to check that headline exists\n",
    "        # we can do that with the handy statement if x is not None: \n",
    "        # whatever is indented will only take place if the 'if' statement evaluates as true; otherwise we'll skip it, and go to the else\n",
    "        if headline is not None:\n",
    "            headline = headline.text.strip()\n",
    "        else:\n",
    "            headline = ''\n",
    "        \n",
    "        # bylines have a lot of variation from page to page so we're going to do a somewhat complex if/else check\n",
    "        byline = page_soup.select('div.article-byline')\n",
    "        if len(byline) > 0:\n",
    "            if byline[0].a is not None:\n",
    "                byline = byline[0].a.text\n",
    "            else:\n",
    "                byline = ''\n",
    "        else:\n",
    "            byline = ''\n",
    "        \n",
    "        # this is a new kind of container object, known as a tuple. It's a good way to package these lists and strings together for each page, and then assign them as one object to the overall results_list. \n",
    "        article_tuple = (url, headline, byline, article_string)\n",
    "        results_list.append(article_tuple)\n",
    "    \n",
    "    #return results_list after the for loop concludes\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call our function, passing the most_read_urls list as a parameter, and saving the return value to a variable called scraped_articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_articles = scrape_strib(most_read_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explore the content by referring to the index for each item in the scraped_articles list. Also, looking over the URLs above, we can guess that some of these (like the videos) are probably not going to fit our articles list very well. When building a scraper, you'll want to spend a fair amount of time testing different kinds of content and building if/else statements to only collect the data you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can look at the full tuple for each article\n",
    "print(scraped_articles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can look at specific items by adding referring to the index of the tuple\n",
    "print('First article:')\n",
    "print('url:', scraped_articles[0][0])\n",
    "print('title:', scraped_articles[0][1])\n",
    "print('byline:', scraped_articles[0][2])\n",
    "print('First 250 chars from the text:', scraped_articles[0][3][0:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data view and export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you've collected a lot of data like this it can be helpful to save it for later use. Which format you want to use to save data in Python depends on the structure of the data, but common formats are CSVs, JSON, and pickle files.\n",
    "\n",
    "In the case of more complex data structures, like our list of lists, we can use a tabular data tool called Pandas to store each article in a row, and then save that Pandas dataframe to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# pandas has a function called DataFrame() that accepts a list as its input, and then we'll define the names of our column names\n",
    "df = pd.DataFrame(scraped_articles, columns=['url', 'title', 'byline', 'text'])\n",
    "\n",
    "# now let's view the table\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can save a dataframe to a csv file using the pandas function to_csv()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('scraped_sites.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More resources\n",
    "\n",
    "- [Programming Historian's Intro to BeautifulSoup](https://programminghistorian.org/en/lessons/intro-to-beautiful-soup)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
